import os
from langchain_openai import AzureChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from unicodedata import normalize

def load_prompt(prompt_name):
    prompt_path = os.path.join('ccs_ai_josh', 'prompts', prompt_name)
    with open(prompt_path, 'r', encoding='utf-8') as file:
        prompt_text = file.read()
    return prompt_text

def score_correctness(llm:AzureChatOpenAI, question:str, generated_answer:str, reference_answer:str) -> int:
    """Uses an LLM-as-a-judge approach to score the similarity between a generated answer and reference answer.
    Args:
        llm: a LangChain LLM connection object
        question: the question from the user/requestor
        generated_answer: the answer generated by the LLM being evaluated
    Returns:
        score: the correctness score, from 1 (poorest) to 10 (best)
    """
    system_prompt = load_prompt('correctness_score_prompt.txt')

    user_prompt = f"""Question: {question}

    Reference answer: {reference_answer}

    Generated answer: {generated_answer}
    """
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]
    response = llm.invoke(messages)
    score_str = response.content.strip()
    try:
        score = int(score_str)
        assert 1 <= score <= 10
    except Exception as e:
        raise ValueError(f"Model returned unexpected score: '{score_str}'") from e
    return score

def score_retrieval(llm:AzureChatOpenAI, question:str, context:list) -> int:
    """Uses an LLM-as-a-judge approach to score the similarity between a generated answer and reference answer.
    Args:
        llm: a LangChain LLM connection object
        question: the question from the user/requestor
        context: the text from the retrieved content, as a list
    Returns:
        score: the correctness score, from 1 (poorest) to 10 (best)
    """
    system_prompt = load_prompt('retrieval_score_prompt.txt')

    user_prompt = f"""Question: {question}

    Retrieved context: {context}

    How likely is it that the retrieved context contains the necessary information to answer the question?
    Reply with a score from 1 (not at all) to 10 (completely covered). Only provide the score.
    """
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]
    response = llm.invoke(messages)
    score_str = response.content.strip()
    try:
        score = int(score_str)
        assert 1 <= score <= 10
    except Exception as e:
        raise ValueError(f"Model returned unexpected score: '{score_str}'") from e
    return score

def score_groundedness(llm:AzureChatOpenAI, context: list, generated_answer: str) -> int:
    """Uses an LLM-as-a-judge approach to score the groundedness of a generated answer with respect to its context.
    Args:
        llm: a LangChain LLM connection object
        context: the text from the retrieved content, as a list
        generated_answer: the answer generated by the LLM being evaluated
    Returns:
        score: the correctness score, from 1 (poorest) to 10 (best)
    """
    system_prompt = load_prompt('groundedness_score_prompt.txt')

    user_prompt = f"""
    Context/retrieved document: {context}

    Generated answer: {generated_answer}

    Score the groundedness of the generated answer in the context from 1 (not grounded) to 10 (fully grounded). 
    Reply with only the score.
    """
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]
    response = llm.invoke(messages)
    score_str = response.content.strip()
    try:
        score = int(score_str)
        assert 1 <= score <= 10
    except Exception as e:
        raise ValueError(f"Model returned unexpected score: '{score_str}'") from e
    return score

def test_doc_match(retrieved_doc:str, ref_doc:str) -> bool:
    """Tests whether a document name matches the reference doc.
    Args:
        retrieved_doc: the name of the document that was retrieved during RAG
        ref_doc: the name of the document that is known to be most relevant
    Returns:
        match_status: whether the retrieved document matches the reference document
    """
    # handling differences in encoding, focusing just on matching string content
    return normalize('NFC', retrieved_doc) == normalize('NFC', ref_doc)

def evaluate_response(llm:AzureChatOpenAI, question:str, answer:str, context:list, retrieved_docs:list, ref_answer:str, ref_doc:str) -> dict:
    """Evaluates a response by calculating correctness, retrieval accuracy and groundedness.
    Args:
        llm: a LangChain LLM connection object
        question: the question from the user
        answer: the answer generated by the LLM being evaluated
        context: the text from the retrieved content, as a list
        retrieved_docs: the names of the files that the content chunks were drawn from
        ref_answer: the known ground truth
        ref_doc: the name of the document that is known to be most relevant to the question
    Returns:
        results: the scores for correctness, retrieval accuracy and groundedness, each from 1 (poorest) to 10 (best)
    """
    correctness_score = score_correctness(
        llm=llm,
        question=question,
        generated_answer=answer,
        reference_answer=ref_answer
    )
    # for cases where retrieval was triggered
    if len(retrieved_docs) > 0:
        retrieval_score = score_retrieval(
            llm=llm,
            question=question,
            context=context
        )
        groundedness_score = score_groundedness(
            llm=llm,
            context=context,
            generated_answer=answer
        )
        document_match = test_doc_match(
            # here we assume that retrieved docs are ordered by most to least relevant
            retrieved_doc=retrieved_docs[0],
            ref_doc=ref_doc
        )
    # for cases where retrieval was not triggered
    else:
        retrieval_score = 0
        groundedness_score = 0
        document_match = False
    results = {
        "correctness": correctness_score,
        "retrieval": retrieval_score,
        "groundedness": groundedness_score,
        "document_match": document_match
    }
    return results
